{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reinforcement Learning\n",
    "Reinforcement learning is a type of machine learning which involves an agent in an environment with a set of states $S$ , and a set of actions $A$. By performing an action $a \\in A$, the environment transitions from state to state. Executing an action in a specific state provides the agent with a reward $r$.\n",
    "\n",
    "The goal of the agent is to maximize its total (future) reward. It does this by adding the maximum reward attainable from future states to the reward for achieving its current state, effectively influencing the current action by the potential future reward. This potential reward is a weighted sum of the expected values of the rewards of all future steps starting from the current state.\n",
    "\n",
    "\n",
    "## Q Learning\n",
    "Q-learning is a model-free reinforcement learning algorithm (ie, no transition probability distribution and reward function). The goal of Q-learning is to learn a policy, which tells an agent what action to take under what circumstances. It does not require a model of the environment, and it can handle problems with stochastic transitions and rewards, without requiring adaptations.\n",
    "\n",
    "For any finite Markov decision process (FMDP), Q-learning finds a policy that is optimal in the sense that it maximizes the expected value of the total reward over any and all successive steps, starting from the current state. Q-learning can identify an optimal action-selection policy for any given FMDP, given infinite exploration time and a partly-random policy. \"Q\" names the function that returns the reward used to provide the reinforcement and can be said to stand for the \"quality\" of an action taken in a given state.\n",
    "\n",
    "**ALGORITHM:**  \n",
    "The weight for a step from a state $\\Delta t$ steps into the future is calculated as $\\gamma^{\\Delta t}$.  \n",
    "where $\\gamma$ (gamma - the discount factor) is a number between 0 and 1. It has the effect of valuing rewards received earlier higher than those received later.\n",
    "\n",
    "The algorithm, therefore, has a function that calculates the quality of a state-action combination:  \n",
    "    $$Q : S \\ast A \\to R$$\n",
    "\n",
    "Before learning begins, $Q$ is initialized to a possibly arbitrary fixed value. Then, at each time $t$ the agent selects an action $a_t$, observes a reward $r_t$, enters a new state $s_{t+1}$ (that may depend on both the previous state $s_t$ and the selected action $a_t$), and $Q$ is updated. The core of the algorithm is a simple value iteration update, using the weighted average of the old value and the new information:\n",
    "\n",
    "   $$Q_{new}(s_t, a_t) \\gets (1-\\alpha).Q(s_t, a_t) + \\alpha.[r_t + \\gamma. max_a Q(s_{t+1}, a)]$$\n",
    "where $r_t$ is the reward received when moving from the state $s_t$ to the state $s_{t+1}$, and $\\alpha$ is the learning rate $( 0 < \\alpha \\leq 1)$.\n",
    "\n",
    "An episode of the algorithm ends when state $s_{t+1}$ is a final or terminal state. However, Q-learning can also learn in non-episodic tasks. If the discount factor is lower than 1, the action values are finite even if the problem can contain infinite loops.\n",
    "\n",
    "For all final states $S_f$,  $Q(s_{f},a)$ is never updated, but is set to the reward value $r$ observed for state $S_f$. In most cases, $Q(s_f, a)$ can be taken to equal zero. \n",
    "\n",
    "\n",
    "**INFLUENCE OF VARIABLES:**\n",
    "* Learning Rate $(\\alpha)$: The learning rate or step size determines to what extent newly acquired information overrides old information.\n",
    "* Explore vs Exploit: Helps in learning optimal policy by first exploring states then exploiting learned Q values.\n",
    "* Discount factor: The discount factor $\\gamma$ determines the importance of future rewards.\n",
    "\n",
    "\n",
    "**IMPLEMENTATION:**  \n",
    "Q-learning at its simplest stores data in tables. This approach falters with increasing numbers of states/actions. \n",
    "\n",
    "* Function approximation: \n",
    "Q-learning can be combined with function approximation. This makes it possible to apply the algorithm to larger problems, even when the state space is continuous.\n",
    "\n",
    "One solution is to use an artificial neural network as a function approximator. Function approximation may speed up learning in finite problems, due to the fact that the algorithm can generalize earlier experiences to previously unseen states. \n",
    "\n",
    "* Quantization:\n",
    "Another technique to decrease the state/action space quantizes possible values.\n",
    "\n",
    "\n",
    "**VARIANTS:**\n",
    "* Deep Q Learning:\n",
    "In this method we will use Deep Neural Networks to estimate function approximation. Reinforcement learning is unstable or divergent when a nonlinear function approximator such as a neural network is used to represent Q. This instability comes from the correlations present in the sequence of observations, the fact that small updates to Q may significantly change the policy and the data distribution, and the correlations between Q and the target values.\n",
    "\n",
    "In order to solve this instability we will use a technique called experience replay, a biologically inspired mechanism that uses a random sample of prior actions instead of the most recent action to proceed. This removes correlations in the observation sequence and smooths changes in the data distribution.\n",
    "\n",
    "* Double Q-learning:\n",
    "Because the future maximum approximated action value in Q-learning is evaluated using the same Q function as in current action selection policy, in noisy environments Q-learning can sometimes overestimate the action values, slowing the learning. A variant called Double Q-learning was proposed to correct this. Double Q-learning is an off-policy reinforcement learning algorithm, where a different policy is used for value evaluation than what is used to select the next action.\n",
    "\n",
    "In practice, two separate value functions are trained in a mutually symmetric fashion using separate experiences, $Q_A$ and $Q_B$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's implement a DQN model to play CartPole game from OpenAI gym\n",
    "\n",
    "* In this game we have 4 real values to represent a state and 2 possible actions (0 = left and 1 = right) in each state.\n",
    "* reward: 1 for every step taken, including the termination step\n",
    "* starting state: All observations are assigned a uniform random value between ±0.05\n",
    "* episode termination: 1.Pole Angle is more than ±12°, 2.Cart Position is more than ±2.4 (center of the cart reaches the edge of the display), 3.Episode length is greater than 200\n",
    "* success: considered solved when the average reward is greater than or equal to 195.0 over 100 consecutive trials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0: mean score=13.0\n",
      "Episode 20: mean score=10.476190476190476\n",
      "Episode 40: mean score=19.51219512195122\n",
      "Episode 60: mean score=31.131147540983605\n",
      "Episode 80: mean score=46.04938271604938\n",
      "Episode 100: mean score=53.08\n",
      "Episode 120: mean score=71.52\n",
      "Episode 140: mean score=89.93\n",
      "Episode 160: mean score=108.7\n",
      "Episode 180: mean score=121.33\n",
      "Episode 200: mean score=134.15\n",
      "Episode 220: mean score=148.23\n",
      "Episode 240: mean score=159.74\n",
      "Episode 260: mean score=161.18\n",
      "Episode 280: mean score=159.56\n",
      "Episode 300: mean score=166.03\n",
      "Episode 320: mean score=167.18\n",
      "Episode 340: mean score=169.28\n",
      "Episode 360: mean score=176.81\n",
      "Episode 380: mean score=186.32\n",
      "Episode 400: mean score=190.13\n",
      "Episode 420: mean score=194.39\n",
      "Ran 427 episodes. Solved after 327 trails\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "327"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# imports\n",
    "import gym\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "\n",
    "\n",
    "# training parameters\n",
    "num_episodes = 1000\n",
    "num_win_ticks = 195\n",
    "max_env_steps = None\n",
    "\n",
    "# discount factor\n",
    "gamma = 1.0\n",
    "# exploration factor\n",
    "epsilon = 1.0\n",
    "epsilon_min = 0.01\n",
    "epsilon_decay = 0.985\n",
    "# learning rate\n",
    "lr = 0.005\n",
    "\n",
    "batch_size = 256\n",
    "monitor = False\n",
    "quiet = False\n",
    "\n",
    "# environment parameters\n",
    "memory = deque(maxlen=5000)\n",
    "env = gym.make('CartPole-v0')\n",
    "if max_env_steps is not None:\n",
    "    env.max_episode_steps = max_env_steps\n",
    "\n",
    "\n",
    "# Building the neural network\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "# model\n",
    "model = Sequential()\n",
    "model.add(Dense(24, input_dim=4, activation='relu'))\n",
    "model.add(Dense(24, activation='relu'))\n",
    "model.add(Dense(2, activation='relu'))\n",
    "\n",
    "model.compile(loss='mse', optimizer=Adam(lr=lr))\n",
    "\n",
    "\n",
    "# define util funtions\n",
    "def remember(state, action, reward, next_state, done):\n",
    "    memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "\n",
    "def choose_action(state, epsilon):\n",
    "    if(np.random.random() <= epsilon):\n",
    "        return env.action_space.sample()\n",
    "    else:\n",
    "        return np.argmax(model.predict(state))\n",
    "\n",
    "\n",
    "def get_epsilon(t):\n",
    "    return max(epsilon_min, min(epsilon, 1.0 - math.log10(t+1) * epsilon_decay))\n",
    "\n",
    "\n",
    "def preprocess_state(state):\n",
    "    return np.reshape(state, [1,4])\n",
    "\n",
    "\n",
    "def replay(batch_size, epsilon):\n",
    "    x_batch = []\n",
    "    y_batch = []\n",
    "    mini_batch = random.sample(memory, min(len(memory), batch_size))\n",
    "    \n",
    "    for state, action, reward, next_state, done in mini_batch:\n",
    "        y_target = model.predict(state)\n",
    "        if(done):\n",
    "            y_target[0][action] = reward\n",
    "        else:\n",
    "            y_target[0][action] = reward + gamma * np.max(model.predict(next_state)[0])\n",
    "        x_batch.append(state[0])\n",
    "        y_batch.append(y_target[0])\n",
    "    \n",
    "    model.fit(np.array(x_batch), np.array(y_batch), batch_size=1, verbose=0)\n",
    "    \n",
    "    if(epsilon > epsilon_min):\n",
    "        epsilon *= epsilon_decay\n",
    "\n",
    "\n",
    "# train function\n",
    "def train():\n",
    "    scores = deque(maxlen=100)\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        state = preprocess_state(env.reset())\n",
    "        done = False\n",
    "        t = 0\n",
    "        while(not done):\n",
    "            action = choose_action(state, get_epsilon(episode))\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            next_state = preprocess_state(next_state)\n",
    "            remember(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            t += 1\n",
    "        scores.append(t)\n",
    "        mean_score = np.mean(scores)\n",
    "        \n",
    "        if(mean_score>=num_win_ticks and episode>=100):\n",
    "            if(not quiet):\n",
    "                print('Ran {} episodes. Solved after {} trails'.format(episode, episode-100))\n",
    "            env.close()\n",
    "            return episode-100\n",
    "        if(episode%20 == 0 and not quiet):\n",
    "            print('Episode {}: mean score={}'.format(episode, mean_score))\n",
    "        \n",
    "        replay(batch_size, get_epsilon(episode))\n",
    "    \n",
    "    if(not quiet):\n",
    "        print('Didn\\'t solve after {} episodes.'.format(episode+1))\n",
    "    env.close()\n",
    "    return episode\n",
    "\n",
    "\n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chance 1: score=200\n",
      "Chance 2: score=200\n",
      "Chance 3: score=200\n",
      "Chance 4: score=200\n",
      "Chance 5: score=200\n"
     ]
    }
   ],
   "source": [
    "def play():\n",
    "    for chance in range(5):\n",
    "        state = preprocess_state(env.reset())\n",
    "        done = False\n",
    "        t = 0\n",
    "        while(not done):\n",
    "            action = np.argmax(model.predict(state))\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            env.render()\n",
    "            next_state = preprocess_state(next_state)\n",
    "            state = next_state\n",
    "            t += 1\n",
    "        print('Chance {}: score={}'.format(chance+1, t))\n",
    "\n",
    "play()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! For all the trials we got a score of 200 which is the maximum score for this game."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
